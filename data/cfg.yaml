text:
  max_len: 512

lstm:
  lstm_hidden_size: 512

classifier:
  classifier_hidden_size: 512
  dropout: 0.25

training:
  batch_size: 32
  lr: 0.00001
  epochs: 300
  val_split_ratio: 0.2
  patience: 30
  max_grad_norm: 5.0
  weight_decay: 0.01

loss_computation: "logits" # "probabilities" or "logits"
samples_debug: 1000  