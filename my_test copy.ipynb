{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144749b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from torchmultimodal.modules.fusions.concat_fusion import ConcatFusionModule\n",
    "from torchmultimodal.modules.encoders.bert_text_encoder import bert_text_encoder\n",
    "from torchmultimodal.models.masked_auto_encoder.model import audio_mae\n",
    "from torchmultimodal.models.masked_auto_encoder.model import MAEOutput\n",
    "\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33171d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the config file and extracting the parameters\n",
    "with open(\"cfg.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "# audio\n",
    "n_mels = config[\"audio\"][\"n_mels\"]\n",
    "target_sample_rate = config[\"audio\"][\"target_sample_rate\"]\n",
    "max_time_steps = config[\"audio\"][\"max_time_steps\"]\n",
    "n_fft = config[\"audio\"][\"n_fft\"]\n",
    "hop_length = config[\"audio\"][\"hop_length\"]\n",
    "\n",
    "# text\n",
    "max_len = config[\"text\"][\"max_len\"]\n",
    "\n",
    "#training\n",
    "batch_size = config[\"training\"][\"batch_size\"]\n",
    "lr = config[\"training\"][\"lr\"]\n",
    "epochs = config[\"training\"][\"epochs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ecb45",
   "metadata": {},
   "source": [
    "## Model  \n",
    "The model is composed of a text pipeline, from Bert architecture, and an audio pipeline, audio masked autoencoder, that get fused together in aconcatenation fusion model. This is then connected with a final linear layer used as classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3890a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/federico/EPFL/DeepLearningProject/multimodal/.venv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "class AudioTextMultimodalModel(nn.Module):\n",
    "    def __init__(self, audio_encoder, text_encoder, fusion_module, output_dim):\n",
    "        super().__init__()\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.fusion_module = fusion_module\n",
    "\n",
    "        # Calculate the input size for the classifier\n",
    "        audio_output_dim = self.audio_encoder.output_dim\n",
    "        text_output_dim = self.text_encoder.output_dim\n",
    "        fusion_output_dim = audio_output_dim + text_output_dim\n",
    "\n",
    "        self.classifier = nn.Linear(fusion_output_dim, output_dim)\n",
    "\n",
    "    def forward(self, audio, input_ids, attention_mask):\n",
    "        audio_output = self.audio_encoder(audio)\n",
    "        # Extract the embeddings tensor from the MAEOutput object and apply pooling.\n",
    "        # audio_output is an MAEOutput NamedTuple. We need the encoder_output field.\n",
    "        # The encoder_output field can be a TransformerOutput or a Tensor.\n",
    "        # If it's a TransformerOutput, we need its last_hidden_state.\n",
    "        if isinstance(audio_output, MAEOutput):\n",
    "            encoder_out = audio_output.encoder_output\n",
    "            if hasattr(encoder_out, 'last_hidden_state') and isinstance(encoder_out.last_hidden_state, torch.Tensor):\n",
    "                # Assuming last_hidden_state is (batch_size, num_patches, embedding_dim)\n",
    "                # Apply mean pooling over the patch dimension (dim=1)\n",
    "                audio_features = encoder_out.last_hidden_state.mean(dim=1)\n",
    "            elif isinstance(encoder_out, torch.Tensor) and encoder_out.ndim == 3:\n",
    "                 # If encoder_output itself is the tensor (batch_size, num_patches, embedding_dim)\n",
    "                 audio_features = encoder_out.mean(dim=1)\n",
    "            else:\n",
    "                 # Fallback or error handling if the encoder_output structure is unexpected\n",
    "                 print(\"Error: Unexpected audio encoder output structure within MAEOutput. Cannot extract features for fusion.\")\n",
    "                 raise TypeError(\"Audio encoder output within MAEOutput is not in an expected format for feature extraction and pooling.\")\n",
    "        elif isinstance(audio_output, torch.Tensor) and audio_output.ndim >= 2:\n",
    "             # If the audio_encoder directly returned a tensor (less likely based on MAEOutput definition, but for robustness)\n",
    "             if audio_output.ndim == 3: # Assuming (batch_size, sequence_length, embedding_dim)\n",
    "                 audio_features = audio_output.mean(dim=1)\n",
    "             else: # Assuming (batch_size, embedding_dim)\n",
    "                 audio_features = audio_output\n",
    "        else:\n",
    "             # Fallback or error handling if the top-level output structure is unexpected\n",
    "             print(\"Error: Unexpected top-level audio encoder output structure. Cannot extract features for fusion.\")\n",
    "             raise TypeError(\"Audio encoder output is not in an expected format for feature extraction and pooling.\")\n",
    "        \n",
    "        text_features = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state.mean(dim=1)  # Average pooling over the sequence length\n",
    "        fused_features = self.fusion_module({\"audio\": audio_features, \"text\": text_features})\n",
    "        return self.classifier(fused_features)\n",
    "\n",
    "# Instantiate components\n",
    "audio_encoder = audio_mae(input_size=(128, 512)) # input size should match your audio data (n_mels, max_time_steps)\n",
    "# text_encoder = CLIPTextEncoder()\n",
    "text_encoder = bert_text_encoder()\n",
    "fusion_module = ConcatFusionModule()\n",
    "model = AudioTextMultimodalModel(audio_encoder, text_encoder, fusion_module, output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87332c74",
   "metadata": {},
   "source": [
    "## Audio Preprocessing\n",
    "The audio dataset is loaded, resampled to a common sampling rate, converted to mono if there is more than one channel, and then transformed in the Mel Spectrogram format, needed by the audio MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fcc6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, target_sample_rate=target_sample_rate):\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    \n",
    "    # Resample if the sample rate is different from the target\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mono (if stereo)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "def preprocess_audio(file_path, target_sample_rate=16000, n_mels=128, max_time_steps=None, n_fft=1024, hop_length=512):\n",
    "    # Load the audio\n",
    "    waveform = load_audio(file_path, target_sample_rate)\n",
    "\n",
    "    # Convert to mel spectrogram\n",
    "    # Ensure the audio tensor is (channels, time) for MelSpectrogram\n",
    "    if waveform.ndim == 1:\n",
    "         waveform = waveform.unsqueeze(0) # Add channel dimension if missing\n",
    "\n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=target_sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    mel_features = mel_spectrogram(waveform)\n",
    "\n",
    "    # Normalize the mel spectrogram\n",
    "    # Add a small epsilon for numerical stability in case of zero std\n",
    "    mean = mel_features.mean()\n",
    "    std = mel_features.std()\n",
    "    mel_features = (mel_features - mean) / (std + 1e-5)\n",
    "\n",
    "    # Pad or truncate to max_time_steps\n",
    "    if max_time_steps is not None:\n",
    "        if mel_features.shape[2] < max_time_steps:\n",
    "            # Pad with zeros\n",
    "            padding = max_time_steps - mel_features.shape[2]\n",
    "            mel_features = torch.nn.functional.pad(mel_features, (0, padding))\n",
    "        else:\n",
    "            # Truncate to max_time_steps\n",
    "            mel_features = mel_features[:, :, :max_time_steps]\n",
    "\n",
    "    return mel_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712f99a",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "The text dataset is tokenized, using the Bert tokenizer, in order to generate the tokens and the attention mask needed by the Bert encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990b841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# Tokenize text\n",
    "def tokenize_text(text, max_len=512):\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",  # Pad to max_len\n",
    "        truncation=True,       # Truncate if longer than max_len\n",
    "        max_length=max_len,    # Maximum sequence length\n",
    "        return_tensors=\"pt\",   # Return PyTorch tensors\n",
    "    )\n",
    "    return tokens[\"input_ids\"].squeeze(0), tokens[\"attention_mask\"].squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5165e6",
   "metadata": {},
   "source": [
    "## Generation of Batched Dataset\n",
    "The collate method generates the dataset uniting preprocessed audio and text relative to the same label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e2c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    audio, input_ids, attention_mask, labels = zip(*batch)\n",
    "    \n",
    "    # Pad audio tensors to the same length\n",
    "    audio = torch.stack(audio)\n",
    "    \n",
    "    # Stack tokenized text and attention masks\n",
    "    input_ids = torch.stack(input_ids)  # Assuming text is already tokenized and of fixed length\n",
    "    attention_mask = torch.stack(attention_mask)\n",
    "\n",
    "    # Convert labels to tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return audio, input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed72c0",
   "metadata": {},
   "source": [
    "## Dataset Class\n",
    "Used for creating a preprocessed dataset from the data available, that can be then used in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28cc172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, audio_files, text_data, labels, audio_transform=None,\n",
    "                 text_transform=None, max_len=512, max_time_steps=512,\n",
    "                 target_sample_rate=16000, n_mels=128, n_fft=1024, hop_length=512):\n",
    "        self.audio_files = audio_files\n",
    "        self.text_data = text_data\n",
    "        self.labels = labels\n",
    "        self.audio_transform = audio_transform\n",
    "        self.text_transform = text_transform\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.max_len = max_len\n",
    "        self.max_time_steps = max_time_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and preprocess audio\n",
    "        audio_path = self.audio_files[idx]\n",
    "        audio = preprocess_audio(audio_path, target_sample_rate=self.target_sample_rate,\n",
    "                                 n_mels=self.n_mels, max_time_steps=self.max_time_steps,\n",
    "                                 n_fft=self.n_fft, hop_length=self.hop_length)  # Preprocess audio\n",
    "        \n",
    "        # Apply additional audio transforms if provided\n",
    "        if self.audio_transform:\n",
    "            audio = self.audio_transform(audio)\n",
    "\n",
    "        # Tokenize and preprocess text\n",
    "        text = self.text_data[idx]\n",
    "        if self.text_transform:\n",
    "            input_ids, attention_mask = self.text_transform(text, max_len=self.max_len)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return audio, input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1ff86",
   "metadata": {},
   "source": [
    "# Generation of Dataloader\n",
    "The data is extracted from its location, preprocessed and inserted in a dataloader that will then be used in the training and in the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b3f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = []  # List of audio file paths or preprocessed tensors\n",
    "text_data = []    # List of text strings\n",
    "labels = []       # List of labels\n",
    "\n",
    "with open('data/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    for entry in data:\n",
    "        audio_files.append(os.path.join('data', entry['file']))\n",
    "        text_data.append(entry['text'])\n",
    "        labels.append(entry['label'])\n",
    "\n",
    "# Instantiate dataset and dataloaders\n",
    "train_dataset = AudioTextDataset(audio_files, text_data, labels, audio_transform=None, text_transform=tokenize_text,\n",
    "                                 max_len=max_len, max_time_steps=max_time_steps,\n",
    "                                 target_sample_rate=target_sample_rate, n_mels=n_mels,\n",
    "                                 n_fft=n_fft, hop_length=hop_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = AudioTextDataset(audio_files, text_data, labels, audio_transform=None, text_transform=tokenize_text,\n",
    "                                 max_len=max_len, max_time_steps=max_time_steps,\n",
    "                                 target_sample_rate=target_sample_rate, n_mels=n_mels,\n",
    "                                 n_fft=n_fft, hop_length=hop_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5376575",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b5defac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for audio, input_ids, attention_mask, labels in tqdm(dataloader):\n",
    "        audio, input_ids, attention_mask, labels = audio.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(audio, input_ids, attention_mask)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for audio, input_ids, attention_mask, labels in tqdm(dataloader):\n",
    "            audio, input_ids, attention_mask, labels = audio.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(audio, input_ids, attention_mask)\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "767bfa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.7846, Val Loss: 0.8973, Val Accuracy: 0.3333\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.81it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0261, Val Loss: 0.5571, Val Accuracy: 0.6667\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.27it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6330, Val Loss: 0.6005, Val Accuracy: 0.6667\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6329, Val Loss: 0.5620, Val Accuracy: 0.6667\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.86it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8027, Val Loss: 0.5772, Val Accuracy: 0.6667\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.70it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5784, Val Loss: 0.5950, Val Accuracy: 0.3333\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.82it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6312, Val Loss: 0.2763, Val Accuracy: 1.0000\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.03it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4023, Val Loss: 0.1761, Val Accuracy: 1.0000\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2486, Val Loss: 0.1063, Val Accuracy: 1.0000\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.88it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0859, Val Loss: 0.0081, Val Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f857d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
